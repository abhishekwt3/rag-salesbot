# test_scraper.py - Test the advanced web scraper capabilities
import asyncio
import logging
import time
from simple_scraper import AdvancedWebScraper

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_static_website():
    """Test scraping a static website"""
    print("\nğŸ§ª Testing Static Website (example.com)")
    scraper = AdvancedWebScraper()
    
    start_time = time.time()
    pages = await scraper.scrape_website(
        'https://example.com',
        {'single_page_mode': True}
    )
    end_time = time.time()
    
    if pages:
        page = pages[0]
        print(f"âœ… Success! Scraped in {end_time - start_time:.2f}s")
        print(f"ğŸ“„ Title: {page['title']}")
        print(f"ğŸ“ Content length: {len(page['content'])} characters")
        print(f"ğŸ”— URL: {page['url']}")
        print(f"ğŸ“Š Preview: {page['content'][:200]}...")
    else:
        print("âŒ Failed to scrape content")

async def test_nextjs_website():
    """Test scraping a Next.js website"""
    print("\nğŸ§ª Testing Next.js Website (nextjs.org)")
    scraper = AdvancedWebScraper()
    
    start_time = time.time()
    pages = await scraper.scrape_website(
        'https://nextjs.org',
        {'single_page_mode': True}
    )
    end_time = time.time()
    
    if pages:
        page = pages[0]
        print(f"âœ… Success! Scraped in {end_time - start_time:.2f}s")
        print(f"ğŸ“„ Title: {page['title']}")
        print(f"ğŸ“ Content length: {len(page['content'])} characters")
        print(f"ğŸ”— URL: {page['url']}")
        print(f"ğŸ“Š Preview: {page['content'][:200]}...")
    else:
        print("âŒ Failed to scrape content")

async def test_react_website():
    """Test scraping a React website"""
    print("\nğŸ§ª Testing React Website (react.dev)")
    scraper = AdvancedWebScraper()
    
    start_time = time.time()
    pages = await scraper.scrape_website(
        'https://react.dev',
        {'single_page_mode': True}
    )
    end_time = time.time()
    
    if pages:
        page = pages[0]
        print(f"âœ… Success! Scraped in {end_time - start_time:.2f}s")
        print(f"ğŸ“„ Title: {page['title']}")
        print(f"ğŸ“ Content length: {len(page['content'])} characters")
        print(f"ğŸ”— URL: {page['url']}")
        print(f"ğŸ“Š Preview: {page['content'][:200]}...")
    else:
        print("âŒ Failed to scrape content")

async def test_multi_page_scraping():
    """Test multi-page scraping with limits"""
    print("\nğŸ§ª Testing Multi-Page Scraping (httpbin.org)")
    scraper = AdvancedWebScraper()
    
    start_time = time.time()
    pages = await scraper.scrape_website(
        'https://httpbin.org',
        {
            'single_page_mode': False,
            'max_pages': 3,  # Limit for testing
            'exclude_patterns': ['/json', '/xml', '/status']
        }
    )
    end_time = time.time()
    
    print(f"âœ… Scraped {len(pages)} pages in {end_time - start_time:.2f}s")
    for i, page in enumerate(pages[:3], 1):
        print(f"  ğŸ“„ Page {i}: {page['title'][:50]}... ({len(page['content'])} chars)")

async def benchmark_scraping_speed():
    """Benchmark scraping speed for different types of sites"""
    print("\nğŸ“Š Benchmarking Scraping Speed")
    
    test_sites = [
        ('Static', 'https://example.com'),
        ('Next.js', 'https://nextjs.org'),
        ('React', 'https://react.dev'),
    ]
    
    scraper = AdvancedWebScraper()
    
    for site_type, url in test_sites:
        try:
            start_time = time.time()
            pages = await scraper.scrape_website(url, {'single_page_mode': True})
            end_time = time.time()
            
            duration = end_time - start_time
            content_length = len(pages[0]['content']) if pages else 0
            
            print(f"  {site_type:8} | {duration:6.2f}s | {content_length:8,} chars")
            
        except Exception as e:
            print(f"  {site_type:8} | ERROR  | {str(e)[:50]}...")

async def main():
    """Run all tests"""
    print("ğŸš€ Advanced Web Scraper Test Suite")
    print("=" * 50)
    
    try:
        # Test different website types
        await test_static_website()
        await test_nextjs_website()
        await test_react_website()
        await test_multi_page_scraping()
        await benchmark_scraping_speed()
        
        print("\nğŸ‰ All tests completed!")
        print("\nğŸ“‹ Summary:")
        print("âœ… Static websites: Supported")
        print("âœ… Next.js applications: Supported")
        print("âœ… React applications: Supported") 
        print("âœ… Multi-page scraping: Supported")
        print("âœ… Dynamic content loading: Supported")
        
        print("\nğŸ’¡ Tips:")
        print("- Use single_page_mode=True for faster scraping")
        print("- Add include/exclude patterns for targeted scraping")
        print("- Increase timeout for very slow websites")
        print("- Monitor memory usage for large scraping jobs")
        
    except Exception as e:
        print(f"\nâŒ Test suite failed: {e}")
        print("ğŸ’¡ Make sure you've run: python setup_browsers.py")

if __name__ == "__main__":
    asyncio.run(main())